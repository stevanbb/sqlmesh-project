Metadata-Version: 2.1
Name: sqlmesh
Version: 0.163.0
Home-page: https://github.com/TobikoData/sqlmesh
Author: TobikoData Inc.
Author-email: engineering@tobikodata.com
License: Apache License 2.0
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: SQL
Classifier: Programming Language :: Python :: 3 :: Only
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: astor
Requires-Dist: click
Requires-Dist: croniter
Requires-Dist: duckdb!=0.10.3
Requires-Dist: dateparser
Requires-Dist: hyperscript>=0.1.0
Requires-Dist: importlib-metadata; python_version < "3.12"
Requires-Dist: ipywidgets
Requires-Dist: jinja2
Requires-Dist: pandas
Requires-Dist: pydantic>=2.0.0
Requires-Dist: requests
Requires-Dist: rich[jupyter]
Requires-Dist: ruamel.yaml
Requires-Dist: setuptools; python_version >= "3.12"
Requires-Dist: sqlglot[rs]~=26.6.0
Requires-Dist: tenacity
Requires-Dist: time-machine
Provides-Extra: athena
Requires-Dist: PyAthena[Pandas]; extra == "athena"
Provides-Extra: azuresql
Requires-Dist: pymssql; extra == "azuresql"
Provides-Extra: bigquery
Requires-Dist: google-cloud-bigquery[pandas]; extra == "bigquery"
Requires-Dist: google-cloud-bigquery-storage; extra == "bigquery"
Provides-Extra: bigframes
Requires-Dist: bigframes>=1.32.0; extra == "bigframes"
Provides-Extra: clickhouse
Requires-Dist: clickhouse-connect; extra == "clickhouse"
Provides-Extra: databricks
Requires-Dist: databricks-sql-connector; extra == "databricks"
Provides-Extra: dev
Requires-Dist: agate==1.7.1; extra == "dev"
Requires-Dist: apache-airflow==2.9.1; extra == "dev"
Requires-Dist: opentelemetry-proto==1.27.0; extra == "dev"
Requires-Dist: beautifulsoup4; extra == "dev"
Requires-Dist: clickhouse-connect; extra == "dev"
Requires-Dist: cryptography; extra == "dev"
Requires-Dist: databricks-sql-connector; extra == "dev"
Requires-Dist: dbt-bigquery; extra == "dev"
Requires-Dist: dbt-core; extra == "dev"
Requires-Dist: dbt-duckdb>=1.7.1; extra == "dev"
Requires-Dist: dbt-snowflake; extra == "dev"
Requires-Dist: dbt-athena-community; extra == "dev"
Requires-Dist: dbt-clickhouse; extra == "dev"
Requires-Dist: dbt-databricks; extra == "dev"
Requires-Dist: dbt-redshift; extra == "dev"
Requires-Dist: dbt-sqlserver>=1.7.0; extra == "dev"
Requires-Dist: dbt-trino; extra == "dev"
Requires-Dist: Faker; extra == "dev"
Requires-Dist: google-auth; extra == "dev"
Requires-Dist: google-cloud-bigquery; extra == "dev"
Requires-Dist: google-cloud-bigquery-storage; extra == "dev"
Requires-Dist: mypy~=1.13.0; extra == "dev"
Requires-Dist: pandas-stubs; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"
Requires-Dist: psycopg2-binary; extra == "dev"
Requires-Dist: pydantic; extra == "dev"
Requires-Dist: PyAthena[Pandas]; extra == "dev"
Requires-Dist: PyGithub~=2.5.0; extra == "dev"
Requires-Dist: pyspark~=3.5.0; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-asyncio; extra == "dev"
Requires-Dist: pytest-mock; extra == "dev"
Requires-Dist: pytest-retry; extra == "dev"
Requires-Dist: pytest-xdist; extra == "dev"
Requires-Dist: pytz; extra == "dev"
Requires-Dist: redshift_connector; extra == "dev"
Requires-Dist: ruff~=0.7.0; extra == "dev"
Requires-Dist: snowflake-connector-python[pandas,secure-local-storage]>=3.0.2; extra == "dev"
Requires-Dist: sqlalchemy-stubs; extra == "dev"
Requires-Dist: trino; extra == "dev"
Requires-Dist: types-croniter; extra == "dev"
Requires-Dist: types-dateparser; extra == "dev"
Requires-Dist: types-PyMySQL; extra == "dev"
Requires-Dist: types-python-dateutil; extra == "dev"
Requires-Dist: types-pytz; extra == "dev"
Requires-Dist: types-requests==2.28.8; extra == "dev"
Requires-Dist: typing-extensions; extra == "dev"
Provides-Extra: dbt
Requires-Dist: dbt-core<2; extra == "dbt"
Provides-Extra: dlt
Requires-Dist: dlt; extra == "dlt"
Provides-Extra: gcppostgres
Requires-Dist: cloud-sql-python-connector[pg8000]>=1.8.0; extra == "gcppostgres"
Provides-Extra: github
Requires-Dist: PyGithub~=2.5.0; extra == "github"
Provides-Extra: llm
Requires-Dist: langchain; extra == "llm"
Requires-Dist: openai; extra == "llm"
Provides-Extra: mssql
Requires-Dist: pymssql; extra == "mssql"
Provides-Extra: mysql
Requires-Dist: pymysql; extra == "mysql"
Provides-Extra: mwaa
Requires-Dist: boto3; extra == "mwaa"
Provides-Extra: postgres
Requires-Dist: psycopg2; extra == "postgres"
Provides-Extra: redshift
Requires-Dist: redshift_connector; extra == "redshift"
Provides-Extra: slack
Requires-Dist: slack_sdk; extra == "slack"
Provides-Extra: snowflake
Requires-Dist: cryptography; extra == "snowflake"
Requires-Dist: snowflake-connector-python[pandas,secure-local-storage]; extra == "snowflake"
Requires-Dist: snowflake-snowpark-python; python_version < "3.12" and extra == "snowflake"
Provides-Extra: trino
Requires-Dist: trino; extra == "trino"
Provides-Extra: web
Requires-Dist: fastapi==0.115.5; extra == "web"
Requires-Dist: watchfiles>=0.19.0; extra == "web"
Requires-Dist: uvicorn[standard]==0.22.0; extra == "web"
Requires-Dist: sse-starlette>=0.2.2; extra == "web"
Requires-Dist: pyarrow; extra == "web"
Provides-Extra: risingwave
Requires-Dist: psycopg2; extra == "risingwave"

<p align="center">
  <img src="https://github.com/TobikoData/sqlmesh/blob/main/docs/readme/sqlmesh.png?raw=true" alt="SQLMesh logo" width="50%" height="50%">
</p>

SQLMesh is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can efficiently run and deploy data transformations written in SQL or Python with visibility and control at any size.

It is more than just a [dbt alternative](https://tobikodata.com/reduce_costs_with_cron_and_partitions.html).

<p align="center">
  <img src="https://github.com/TobikoData/sqlmesh/blob/main/docs/readme/architecture_diagram.png?raw=true" alt="Architecture Diagram" width="100%" height="100%">
</p>

## Core Features
<img src="https://github.com/TobikoData/sqlmesh-public-assets/blob/main/sqlmesh_plan_mode.gif?raw=true" alt="SQLMesh Plan Mode">

> Get instant SQL impact analysis of your changes, whether in the CLI or in [SQLMesh Plan Mode](https://sqlmesh.readthedocs.io/en/stable/guides/ui/?h=modes#working-with-an-ide)

  <details>
  <summary><b>Virtual Data Environments</b></summary>

  * See a full diagram of how [Virtual Data Environments](https://whimsical.com/virtual-data-environments-MCT8ngSxFHict4wiL48ymz) work
  * [Watch this video to learn more](https://www.youtube.com/watch?v=weJH3eM0rzc)

  </details>

  * Create isolated development environments without data warehouse costs
  * Plan / Apply workflow like [Terraform](https://www.terraform.io/) to understand potential impact of changes
  * Easy to use [CI/CD bot](https://sqlmesh.readthedocs.io/en/stable/integrations/github/) for true blue-green deployments

<details>
<summary><b>Efficiency and Testing</b></summary>

Running this command will generate a unit test file in the `tests/` folder: `test_stg_payments.yaml`

Runs a live query to generate the expected output of the model

```bash
sqlmesh create_test tcloud_demo.stg_payments --query tcloud_demo.seed_raw_payments "select * from tcloud_demo.seed_raw_payments limit 5"

# run the unit test
sqlmesh test
```

```sql
MODEL (
  name tcloud_demo.stg_payments,
  cron '@daily',
  grain payment_id,
  audits (UNIQUE_VALUES(columns = (
      payment_id
  )), NOT_NULL(columns = (
      payment_id
  )))
);

SELECT
    id AS payment_id,
    order_id,
    payment_method,
    amount / 100 AS amount, /* `amount` is currently stored in cents, so we convert it to dollars */
    'new_column' AS new_column, /* non-breaking change example  */
FROM tcloud_demo.seed_raw_payments
```

```yaml
test_stg_payments:
model: tcloud_demo.stg_payments
inputs:
    tcloud_demo.seed_raw_payments:
      - id: 66
        order_id: 58
        payment_method: coupon
        amount: 1800
      - id: 27
        order_id: 24
        payment_method: coupon
        amount: 2600
      - id: 30
        order_id: 25
        payment_method: coupon
        amount: 1600
      - id: 109
        order_id: 95
        payment_method: coupon
        amount: 2400
      - id: 3
        order_id: 3
        payment_method: coupon
        amount: 100
outputs:
    query:
      - payment_id: 66
        order_id: 58
        payment_method: coupon
        amount: 18.0
        new_column: new_column
      - payment_id: 27
        order_id: 24
        payment_method: coupon
        amount: 26.0
        new_column: new_column
      - payment_id: 30
        order_id: 25
        payment_method: coupon
        amount: 16.0
        new_column: new_column
      - payment_id: 109
        order_id: 95
        payment_method: coupon
        amount: 24.0
        new_column: new_column
      - payment_id: 3
        order_id: 3
        payment_method: coupon
        amount: 1.0
        new_column: new_column
```
</details>

* Never build a table [more than once](https://tobikodata.com/simplicity-or-efficiency-how-dbt-makes-you-choose.html)
* Track what dataâ€™s been modified and run only the necessary transformations for [incremental models](https://tobikodata.com/correctly-loading-incremental-data-at-scale.html)
* Run [unit tests](https://tobikodata.com/we-need-even-greater-expectations.html) for free and configure automated audits

<details>
<summary><b>Level Up Your SQL</b></summary>
Write SQL in any dialect and SQLMesh will transpile it to your target SQL dialect on the fly before sending it to the warehouse.
<img src="https://github.com/TobikoData/sqlmesh/blob/main/docs/readme/transpile_example.png?raw=true" alt="Transpile Example">
</details>

* Debug transformation errors *before* you run them in your warehouse in [10+ different SQL dialects](https://sqlmesh.readthedocs.io/en/stable/integrations/overview/#execution-engines)
* Definitions using [simply SQL](https://sqlmesh.readthedocs.io/en/stable/concepts/models/sql_models/#sql-based-definition) (no need for redundant and confusing `Jinja` + `YAML`)
* See impact of changes before you run them in your warehouse with column-level lineage

For more information, check out the [website](https://sqlmesh.com) and [documentation](https://sqlmesh.readthedocs.io/en/stable/).

## Getting Started
Install SQLMesh through [pypi](https://pypi.org/project/sqlmesh/) by running:

```bash
mkdir sqlmesh-example
cd sqlmesh-example
python -m venv .venv
source .venv/bin/activate
pip install sqlmesh
source .venv/bin/activate # reactivate the venv to ensure you're using the right installation
sqlmesh init duckdb # get started right away with a local duckdb instance
sqlmesh plan # see the plan for the changes you're making
```

> Note: You may need to run `python3` or `pip3` instead of `python` or `pip`, depending on your python installation.

Follow the [quickstart guide](https://sqlmesh.readthedocs.io/en/stable/quickstart/cli/#1-create-the-sqlmesh-project) to learn how to use SQLMesh. You already have a head start!

Follow this [example](https://sqlmesh.readthedocs.io/en/stable/examples/incremental_time_full_walkthrough/) to learn how to use SQLMesh in a full walkthrough.

## Join Our Community
Together, we want to build data transformation without the waste. Connect with us in the following ways:

* Join the [Tobiko Slack Community](https://tobikodata.com/slack) to ask questions, or just to say hi!
* File an issue on our [GitHub](https://github.com/TobikoData/sqlmesh/issues/new)
* Send us an email at [hello@tobikodata.com](mailto:hello@tobikodata.com) with your questions or feedback
* Read our [blog](https://tobikodata.com/blog)

## Contribution
Contributions in the form of issues or pull requests (from fork) are greatly appreciated. 

[Read more](https://sqlmesh.readthedocs.io/en/stable/development/) on how to contribute to SQLMesh open source.

[Watch this video walkthrough](https://www.loom.com/share/2abd0d661c12459693fa155490633126?sid=b65c1c0f-8ef7-4036-ad19-3f85a3b87ff2) to see how our team contributes a feature to SQLMesh.
